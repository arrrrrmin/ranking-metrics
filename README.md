# ranking-metrics

> A repository to understand ranking metrics as described by Musgrave et al. (2020).

This implementation is not the a very fast one, but imho nice to read for 
understanding what's going on. Anyhow improvements are very welcome.

Resources used:
* [faiss getting started guide](https://github.com/facebookresearch/faiss/wiki/Getting-started)

The following examples replicate the toy example of Musgrave et al. in 
[*A Metric Learning Reality Check*](https://arxiv.org/abs/2003.08505). 
Plots are generated by running the tests in 
[test_reality_check.py](tests/test_reality_check.py).

![Example 1](figures/reality_check_example1.png)
> Replicated the example 1 of Musgrave et al. "A Metric Learning Reality Check".

![Example 2](figures/reality_check_example2.png)
> Replicated the example 2 of Musgrave et al. "A Metric Learning Reality Check".

![Example 3](figures/reality_check_example3.png)
> Replicated the example 2 of Musgrave et al. "A Metric Learning Reality Check".

The code for calculating the metrics can be found in 
[embed_metrics.py](src/ranking_metrics/embed_metrics.py) and thanks to faiss it's 
just a 100 lines. Faiss takes care of finding the nearest neighbors for a query.

## Professional implementations 

* [powerful-benchmarker](https://github.com/KevinMusgrave/powerful-benchmarker)
* [pytorch-metric-learning](https://github.com/KevinMusgrave/pytorch-metric-learning)
* [probabilistic-embeddings](https://github.com/tinkoff-ai/probabilistic-embeddings/tree/main/src/probabilistic_embeddings/metrics)
